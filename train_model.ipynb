{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41bb7faf-a747-4c78-812c-b384af91f7f1",
   "metadata": {},
   "source": [
    "## 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "011dcf5b-2a47-41af-b56f-1663deac7fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fafd6f-a38a-4a1d-a142-2f41fe729653",
   "metadata": {},
   "source": [
    "## 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42bca2c-1cb7-4a7b-936a-77ff1e20cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a6aec2-4287-4be8-ad53-60f681653c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "992ac4bf-c8bf-4745-92a0-94e8cde94508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(images, results):\n",
    "    mp_drawing.draw_landmarks(images, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(images, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "316b5298-6925-4d44-8038-345a85f87f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(images, results):\n",
    "    \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(images, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    \n",
    "    # Draw right hand connections    \n",
    "    mp_drawing.draw_landmarks(images, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d9b69c7-8e02-4fee-8ad4-ee3793e16a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=.5, min_tracking_confidence=.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # Flip images\n",
    "        image = cv2.flip(image, 1)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q' or 'Q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b65448-d095-4aa1-9db5-66001d5065b4",
   "metadata": {},
   "source": [
    "## 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f88327c-96cb-4a0d-9e83-2bb8ce8a9b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e785375c-3a39-483d-a0f5-d0b6c8c542ad",
   "metadata": {},
   "source": [
    "## 4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceb6a770-3f9d-4fa2-8c1e-9cf02989a556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path ofr exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data_test')\n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['nothing', 'add_count', 'reset_count'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 40\n",
    "\n",
    "# Video are going to be 30 frames in length\n",
    "sequence_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daf2a421-b0ad-4227-bede-93abcc14c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e5c329-fe43-4bf7-be38-c960a9429e6e",
   "metadata": {},
   "source": [
    "## 5. Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2addf11d-e5f5-445d-bdf2-a3bd26370a9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15616/649756752.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# Set mediapipe model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mmp_holistic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHolistic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_detection_confidence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_tracking_confidence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mholistic\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# New Loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=.5, min_tracking_confidence=.5) as holistic:\n",
    "    \n",
    "    # New Loop\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            # Loop through video lenght aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # Apply wait logic\n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120, 200),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {0} Video Number {1}'.format(action, sequence), (15, 12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else:\n",
    "                    cv2.putText(image, 'Collecting frames for {0} Video Number {1}'.format(action, sequence), (15, 12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = hand_extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "                \n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q' or 'Q'):\n",
    "                    break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e880d6-e492-434c-9819-9d780af63406",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca5b3b-359e-49a5-8a94-147febcfdf97",
   "metadata": {},
   "source": [
    "## 6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09706f7c-dae1-4c23-8d6e-ddc1f153a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ba3ea92-e27b-4cbc-b21a-8904e6651734",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56b4c030-2f60-4627-bc09-bcfb22f3460b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nothing': 0, 'add_count': 1, 'reset_count': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c73f16b-ca3f-4eba-a03b-1f9302b90183",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), '{}.npy'.format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e85cfa-0a89-4509-9dbc-fb136f7a50cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 20, 126)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7facf30a-4d11-4f5e-aeaf-e14197fa819c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 20, 126)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c705ac-bce5-427e-b741-f10fd7747ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4caffd8-c69a-4e41-af98-47a251874aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2079cb-027b-4e99-bc36-4c2eeda342d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 20, 126)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "119b0e36-ca70-486a-9bd8-2ada4145596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d892f93-72e1-4e4a-8fa5-41e36bbca574",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b84dfaae-640d-4dd5-9504-7e4cea7c98ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12, 20, 126), (12, 3))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486c6ce-b99c-4931-bc0a-79ff71d2b2f3",
   "metadata": {},
   "source": [
    "## 7. Build and Train LSTM Neural Networkto_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcaf12d5-75fc-41f1-baf7-f446b01d9548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense, Input\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3c0bdac-2127-4d83-8940-d7ee5ab3ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs_RNN')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "286225dd-e94f-4e96-a6be-7cc1c34192f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "input_shape = Input(shape=(20, 126))\n",
    "layer = LSTM(64, return_sequences=True, activation='relu')(input_shape)\n",
    "layer = LSTM(128, return_sequences=True, activation='relu')(layer)\n",
    "layer = LSTM(64, return_sequences=False, activation='relu')(layer)\n",
    "layer = Dense(64, activation='relu')(layer)\n",
    "layer = Dense(32, activation='relu')(layer)\n",
    "output = Dense(actions.shape[0], activation='softmax')(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ca9a4b1-5d5c-4932-a46a-c316afdfded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = Input(shape=(20, 126))\n",
    "layer = SimpleRNN(64, return_sequences=True, activation='relu')(input_shape)\n",
    "layer = SimpleRNN(128, return_sequences=True, activation='relu')(layer)\n",
    "layer = SimpleRNN(64, return_sequences=False, activation='relu')(layer)\n",
    "layer = Dense(64, activation='relu')(layer)\n",
    "layer = Dense(32, activation='relu')(layer)\n",
    "output = Dense(actions.shape[0], activation='softmax')(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5f323f5-b184-422f-9461-fd072ff37157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Model(input_shape, output, name='LSTM')\n",
    "model = Model(input_shape, output, name='RNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc45b6a1-46cc-49ac-92a9-da3d2a583a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20, 126)]         0         \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 20, 64)            12224     \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 20, 128)           24704     \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 64)                12352     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 55,619\n",
      "Trainable params: 55,619\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6392ec14-5962-41d8-9903-c4e32d96b0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ff0be6f-ea6d-4f5d-956a-ecb78ec238af",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint('best-model_RNN.h5')\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "962db380-7369-41ad-bd6b-5b1f49adc2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 1.1353 - categorical_accuracy: 0.4375WARNING:tensorflow:From C:\\Users\\hoya9\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "2/4 [==============>...............] - ETA: 0s - loss: 1.1610 - categorical_accuracy: 0.3281WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0439s vs `on_train_batch_end` time: 0.1519s). Check your callbacks.\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 1.1092 - categorical_accuracy: 0.3889\n",
      "Epoch 2/30\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.9915 - categorical_accuracy: 0.5278\n",
      "Epoch 3/30\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.8561 - categorical_accuracy: 0.7778\n",
      "Epoch 4/30\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.7051 - categorical_accuracy: 0.8056\n",
      "Epoch 5/30\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4692 - categorical_accuracy: 0.8796\n",
      "Epoch 6/30\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4323 - categorical_accuracy: 0.7870\n",
      "Epoch 7/30\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2710 - categorical_accuracy: 0.9259\n",
      "Epoch 8/30\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3689 - categorical_accuracy: 0.8333\n",
      "Epoch 9/30\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2649 - categorical_accuracy: 0.9167\n",
      "Epoch 10/30\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2866 - categorical_accuracy: 0.9074\n",
      "Epoch 11/30\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2936 - categorical_accuracy: 0.8981\n",
      "Epoch 12/30\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2124 - categorical_accuracy: 0.9259\n",
      "Epoch 13/30\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.1823 - categorical_accuracy: 0.9444\n",
      "Epoch 14/30\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.1217 - categorical_accuracy: 0.9630\n",
      "Epoch 15/30\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.1489 - categorical_accuracy: 0.9352\n",
      "Epoch 16/30\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.1600 - categorical_accuracy: 0.9352\n",
      "Epoch 17/30\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0843 - categorical_accuracy: 0.9722\n",
      "Epoch 18/30\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2175 - categorical_accuracy: 0.9444\n",
      "Epoch 19/30\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.1059 - categorical_accuracy: 0.9444\n",
      "Epoch 20/30\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.1154 - categorical_accuracy: 0.9537\n",
      "Epoch 21/30\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0987 - categorical_accuracy: 0.9630\n",
      "Epoch 22/30\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0815 - categorical_accuracy: 0.9722\n",
      "Epoch 23/30\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0839 - categorical_accuracy: 0.9630\n",
      "Epoch 24/30\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0761 - categorical_accuracy: 0.9722\n",
      "Epoch 25/30\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0863 - categorical_accuracy: 0.9630\n",
      "Epoch 26/30\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0772 - categorical_accuracy: 0.9630\n",
      "Epoch 27/30\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.0681 - categorical_accuracy: 0.9722\n",
      "Epoch 28/30\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.0691 - categorical_accuracy: 0.9722\n",
      "Epoch 29/30\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0694 - categorical_accuracy: 0.9722\n",
      "Epoch 30/30\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0637 - categorical_accuracy: 0.9722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14823c2ec70>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=30, callbacks=[tb_callback, checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf1fb76-93a4-402f-aa0b-b2dc408625a2",
   "metadata": {},
   "source": [
    "## 9. Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80ecd0e8-46a6-43e0-bdfc-201b0957ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('C:/Users/hoya9/Desktop/CV/MediaPipe/best-model_LSTM.h5')\n",
    "model = load_model('C:/Users/hoya9/Desktop/CV/MediaPipe/best-model_RNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b86d640-63c8-494e-9b9e-cf3ed5377fee",
   "metadata": {},
   "source": [
    "## 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0db41b86-f984-4c39-ad8b-36e198745344",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1a460bd-af2d-4099-82ae-c4a1feb78ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'add_count'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e430e8d5-3033-4c00-a0a8-570045783ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'add_count'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c44c8-296e-422b-9d7f-75462cb18d84",
   "metadata": {},
   "source": [
    "## 10. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2702f8b-b228-4333-b174-6507f91d6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c064d581-d98e-471b-b3b9-5bac28750915",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35688490-c91b-4e50-8acf-742acd5741cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80671a26-695f-4863-85fd-a9d698b06cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[9, 0],\n",
       "        [0, 3]],\n",
       "\n",
       "       [[7, 0],\n",
       "        [0, 5]],\n",
       "\n",
       "       [[8, 0],\n",
       "        [0, 4]]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9d9a2ad-8ccd-403f-8114-a47f4c99c11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e592d96-4000-4656-b4d7-72becf4cabd2",
   "metadata": {},
   "source": [
    "## 11. Test Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf6b1c61-78e5-4339-8e92-f153be1d59f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245, 117, 16), (117, 245, 16), (16, 117, 245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0, 60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8b43a78-a5e5-479c-a1db-3630b96210ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.7\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = hand_extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-20:]\n",
    "        \n",
    "        if len(sequence) == 20:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 3: \n",
    "                sentence = sentence[-3:]\n",
    "                \n",
    "            image = cv2.flip(image,1)\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "        \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (0, 0, 0), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f96ff188-181f-4f7c-8b28-915204988302",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89ed45b-b940-4acd-a682-83374c25ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ebb215-f998-401c-97df-7ac9030fa450",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(x_test[0], axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273bf0a5-bbae-49b1-bf5c-9b573186e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[np.argmax(res)] > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a766649c-6495-4aa7-a2ec-43ba6eb9e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.expand_dims(x_test[0], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb81531-3586-4cbf-8887-fba5d77bbaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[Tensorflow2.3-GPU]",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
